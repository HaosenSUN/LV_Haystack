<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>[[PAGE_TITLE]]</title>
    <style>
        body {
        font-family: Arial, sans-serif;
        background: #f9f9f9;
        /* 整体字体稍微缩小 */
        font-size: 14px;
        }
        .infographic-container {
        display: flex;
        flex-wrap: wrap;
        gap: 16px;
        justify-content: center;
        align-items: flex-start;
        }
        .info-card {
        background: #fff;
        border-radius: 10px;
        box-shadow: 0 2px 6px rgba(0,0,0,0.1);
        width: 300px;
        padding: 20px;
        text-align: left;
        }
        .info-card img {
        display: block;       /* 变成块级元素以便使用 margin:0 auto */
        margin: 0 auto 8px;   /* 水平居中 + 设置8px底部外边距 */
        width: 40px;
        height: 40px;
        object-fit: contain;
        }
        .info-title {
        margin: 8px 0 4px;
        font-size: 16px; /* 原先 18px，稍降 */
        font-weight: bold;
        text-align: center;
        }
        .info-subtitle {
        color: #777;
        margin-bottom: 10px;
        font-size: 14px;
        text-align: center;
        }
        .info-card p {
        line-height: 0.5;   

        /* 字体颜色（示例 #777 比 #000 浅一些）*/
        color: #777;        

        margin: 4px 0;      
        }

        #abstract h1, #abstract p {
            color: #000000;
            text-align: justify;
            text-justify: inter-word; 
        }
        h1 {
            color: #4E2A84;
        }
        h2 {
            color: #2c3e50;
        }
        ul {
            list-style-type: disc;
            margin-left: 20px;
        }
        li {
            margin-bottom: 10px;
        }
        .summary {
            margin-top: 20px;
            font-weight: bold;
        }
        .author-container {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 10px;
            justify-content: center;
            align-items: center;
            margin-top: 10px;
        }

        .author-name a {
            text-decoration: none;
            color: #000000;
            font-size: 14px;
        }

        .author-name a:hover {
            text-decoration: underline;
        }
        .slideshow-container {
        position: relative;
        max-width: 100%;
        margin: auto;
        overflow: hidden; /* 若有视频溢出，隐藏 */
        }

        /* 每个幻灯片 */
        .mySlides {
        display: flex;
        gap: 20px;
        justify-content: center;
        align-items: flex-start; /* 如果高度有差异，可让顶部对齐 */
        }

        .mySlides > div {
        flex: 1;               /* 两列平均分 */
        max-width: 600px;      /* 可以给个最大宽度限制 */
        box-sizing: border-box;
        }

        .mySlides video {
        width: 100%;
        height: auto;
        vertical-align: middle; /* 防止底部出现几像素的空隙 */
        }

        /* 幻灯片内文字 */
        .text {
        /* 如果之前有下述属性，可暂时注释掉或移除 */
        /* white-space: nowrap;
        text-overflow: ellipsis;
        overflow: hidden; */

        white-space: normal;   
        overflow: visible;  
        }

        /* 上/下一个 按钮 */
        .prev, .next {
        cursor: pointer;
        position: absolute;
        top: 50%;
        width: auto;
        padding: 16px;
        margin-top: -22px;
        color: white;
        font-weight: bold;
        font-size: 18px;
        border-radius: 0 3px 3px 0;
        user-select: none;
        z-index: 1;
        background-color: rgba(0, 0, 0, 0.5); /* 半透明背景 */
        }
        .next {
        right: 0;
        border-radius: 3px 0 0 3px;
        }
        .prev:hover, .next:hover {
        background-color: rgba(0, 0, 0, 0.8);
        }

        /* 小圆点指示器样式 */
        .dot {
        cursor: pointer;
        height: 15px; 
        width: 15px; 
        margin: 0 2px; 
        background-color: #bbb; 
        border-radius: 50%;
        display: inline-block; 
        transition: background-color 0.6s ease;
        }
        .active, .dot:hover {
        background-color: #717171;
        p {
        text-align: justify; /* 两边对齐 */
        }
        .full-page-image {
        position: relative;
        width: 100%;
        min-height: 100px; 
        /* 如果想保持窗口全屏高度，可用 height: 100vh; */
        /* height: 100vh; */
        overflow: hidden;  /* 防止子元素溢出 */
        }

        .full-page-image img {
        display: block;         /* 避免默认内联元素空隙 */
        width: 100%;            /* 让图片随父容器宽度自适应 */
        height: auto;           /* 等比例缩放，不至于溢出 */
        object-fit: cover;      /* 如果想让图片完全覆盖容器，但可能截取部分 */
        }

        .overlay {
        position: absolute; 
        top: 0; left: 0; 
        right: 0; bottom: 0;
        background: rgba(0, 0, 0, 0.3); /* 半透明蒙层示例 */
        }

        .content {
        position: relative; /* 确保在 overlay 之上 */
        z-index: 1;         
        color: #fff;        /* 文字颜色看需求 */
        /* 其他排版、间距等 */
        }

    }
    </style>
    <link rel="stylesheet" href="assets/css/main.css">
    <link rel="apple-touch-icon" sizes="180x180" href="[[ICON_PNG]]">
    <link rel="icon" type="image/png" sizes="32x32" href="[[ICON_PNG]]">
    <link rel="icon" type="image/png" sizes="16x16" href="[[ICON_PNG]]">
    <link rel="manifest" href="/site.webmanifest">

    <meta property="og:type" content="website"/>
    <meta property="og:image" content="[[ICON_PNG]]"/>
    <meta property="og:image:type" content="image/png">
    <meta property="og:url" content="[[OG_URL]]"/>
    <meta property="og:title" content="[[OG_TITLE]]"/>
    <meta property="og:description" content="[[OG_DESCRIPTION]]"/>

    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:title" content="[[TWITTER_TITLE]]"/>
    <meta name="twitter:description" content="[[TWITTER_DESCRIPTION]]"/>
    <meta name="twitter:creator" content="[[TWITTER_CREATOR]]"/>

    <meta name="twitter:label1" content="Published at"/>
    <meta name="twitter:data1" content="[[PUBLISHED_AT]]"/>
    <meta name="twitter:label2" content="Reading time"/>
    <meta name="twitter:data2" content="[[READING_TIME]]"/>
</head>
<body>
<!-- <div class="full-page-image">
    <video id="bg-video" autoplay loop muted playsinline>
        <source src="/Users/sunhaosen/Desktop/LV-Haystack/assets/img/Demo_0108_v3.mp4" type="video/mp4">
    </video>
    <div class="overlay"></div>
    <div class="content" style="padding: 0 20px">
        <h1>Re-thinking Temporal Search for Long-Form Video Understanding</h1>
        <p>[[MAIN_SUBTEXT]]</p>
    </div>
</div> -->
<div class="full-page-image">
    <img
      src="assets/img/Demo_0108_v3.gif"
      alt="full screen gif"
      style="width: 100%; height: 100%; object-fit: cover;"
    >
    <div class="overlay"></div>
    <div class="content" style="padding: 0 20px">
        <h1>Re-thinking Temporal Search for Long-Form Video Understanding</h1>
        <p>[[MAIN_SUBTEXT]]</p>
    </div>
  </div>

<div id="title_slide">
    <div class="title_left">
        
        <h1 style="color:#4E2A84;"> Re-thinking Temporal Search for Long-Form Video Understanding </h1>
            <!-- <br> -->
        <!-- <div class="affiliation"> -->
        <!-- <p> <b> This is the template of NU-MLL-Lab paper websites. </b> </p>
        <p> Please check index.html and make sure <b>all content inside "[[]]" are replaced</b> with 
            the content of the paper or have been removed.</p>
        <p> after that, please <b>check all hyperlinks</b> by CTRL+F "src" and "href" in the html file and make sure they are correctly placed. </p>
        <p> please refer to 
            <a href="https://transic-robot.github.io/" style="color:#4E2AFF;">this website</a> 
            or 
            <a href="/index_old.html" style="color:#4E2AFF;">index_old.html</a> in this folder for the original code if you feel not sure what to fill in the pleaceholders. </p> -->
        <!-- </div class="affiliation"> -->
        <div class="author-container">
            <div class="author-name"><a href="https://jhuiye.com/" target="_blank">Jinhui Ye<sup>1 </a></div>
            <div class="author-name"><a href="https://zihanwang314.github.io/" target="_blank">Zihan Wang<sup>2 </a></div>
            <div class="author-name"><a href="https://haosensun.github.io/" target="_blank">Haosen Sun<sup>2 </a></div>
            <div class="author-name"><a href="https://keshik6.github.io/" target="_blank">Keshigeyan Chandrasegaran<sup>1 </a></div>
            <div class="author-name"><a href="https://zanedurante.github.io/" target="_blank">Zane Durante<sup>1 </a></div>
            <div class="author-name"><a href="https://ceyzaguirre4.github.io/" target="_blank">Cristobal Eyzaguirre<sup>1 </a></div>
            <div class="author-name"><a href="https://talkingtorobots.com/yonatanbisk.html" target="_blank">Yonatan Bisk<sup>3 </a></div>
            <div class="author-name"><a href="https://www.niebles.net/" target="_blank">Juan Carlos Niebles<sup>1 </a></div>
            <div class="author-name"><a href="https://profiles.stanford.edu/ehsan-adeli" target="_blank">Ehsan Adeli<sup>1 </a></div>
            <div class="author-name"><a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei<sup>1 </a></div>
            <div class="author-name"><a href="https://jiajunwu.com/" target="_blank">Jiajun Wu<sup>1 </a></div>
            <div class="author-name"><a href="https://limanling.github.io/" target="_blank">Manling Li<sup>2 </a></div>
            
        </div>
        <br>
        <p class="affiliation">
            <!-- <sup>2</sup>The Hong Kong University of Science and Technology (Guangzhou) &nbsp;  -->
            <sup>1</sup>Stanford University &nbsp; 
            <sup>2</sup>Northwestern University &nbsp; 
            <sup>3</sup>Carnegie Mellon University &nbsp; 
            <!-- <sup>†</sup>Equal Contribution -->
        </p>
        <br>
        <!-- <div class="affiliation">
            <p><img src="assets/logos/Horizontal_purple_RGB.png" style="height: 20px"></p>
        </div>
        <br>
        <div class="venue">
            <p><b>Conference on Computer Vision and Pattern Recognition (CVPR) 2025</b></p>
        </div> -->
        <br>
        <div class="button-container">
            <a href="[[LINK]]" target="_blank" class="button"><i class="ai ai-arxiv"></i>&emsp14;arXiv</a>
            <a href="assets/pdf/5448_Re_thinking_Temporal_Sear.pdf" target="_blank" class="button"><i class="fa-light fa-file"></i>&emsp14;PDF</a>
            <a href="[[LINK]]" target="_blank" class="button"><i
                    class="fa-brands fa-x-twitter"></i>&emsp14;tl;dr</a>
            <a href="[[LINK]]" target="_blank" class="button"><i
                    class="fa-light fa-code"></i>&emsp14;Code</a>
            <!-- <a href="[[LINK]]" target="_blank" class="button"><i
                    class="fa-light fa-gear-code"></i>&emsp14;Sim Code</a> -->
            <a href="http://mll-4090-3.cs.northwestern.edu:8082/" target="_blank" class="button"><i
                    class="fa-light fa-robot-astromech"></i>&emsp14;Demo</a>
            <a href="https://huggingface.co/LVHaystack" target="_blank" class="button"><i
                    class="fa-light fa-face-smiling-hands"></i>&emsp14;Data</a>
        </div>
        <!-- <br>
        <div class="slideshow-container">
            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/pull/grid.mp4" type="video/mp4">
                </video>
                <div class="text">[[SLIDESHOW_TEXT_1]]
                </div>
            </div>

            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/pull/square_table.mp4" type="video/mp4">
                </video>
                <div class="text">[[SLIDESHOW_TEXT_2]]</div>
            </div>
        </div>
        
        <br> -->
        <!-- <div class="slideshow-container">
            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/pull/grid.mp4" type="video/mp4">
                </video>
                <div class="text">T<span style="font-variant-caps:all-small-caps;">RANSIC</span> for sim-to-real
                    transfer of robot manipulation policies.
                </div>
            </div>

            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/pull/square_table.mp4" type="video/mp4">
                </video>
                <div class="text">Assemble a square table.</div>
            </div>
            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/pull/square_table.mp4" type="video/mp4">
                </video>
                <div class="text">Assemble a square table.</div>
            </div>
            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/pull/square_table.mp4" type="video/mp4">
                </video>
                <div class="text">Assemble a square table.</div>
            </div>
            <a class="prev" onclick="plusSlides(-1)">&#10094;</a>
            <a class="next" onclick="plusSlides(1)">&#10095;</a>
        </div>
        <div style="text-align:center">
            <span class="dot" onclick="currentSlide(1)"></span>
            <span class="dot" onclick="currentSlide(2)"></span>
            <span class="dot" onclick="currentSlide(3)"></span>
            <span class="dot" onclick="currentSlide(4)"></span>
        </div> -->
        
        <div class="slideshow-container">
  
            <!-- 幻灯片 1：并排显示两个视频 -->
            <div class="mySlides fade">
              <!-- 视频1 -->
              <div>
                <img
                    src="assets/img/distribution1.gif"
                    alt="full screen gif"
                    style="width: 100%; height: 100%; object-fit: cover;"
                >
                <!-- <video autoplay muted playsinline loop preload="metadata" width="100%">
                  <source src="assets/videos/pull/grid.mp4" type="video/mp4">
                </video> -->
                <div class="text">Demo 1: What color was the hanged jacket?</div>
              </div>
              <!-- 视频2 -->
              <div>
                <img
                    src="/Users/sunhaosen/Desktop/LV-Haystack/assets/img/what color was the hanged jacket_.gif"
                    alt="full screen gif"
                    style="width: 100%; height: 100%; object-fit: cover;"
                >
                <!-- <video autoplay muted playsinline loop preload="metadata" width="100%">
                  <source src="assets/videos/pull/square_table.mp4" type="video/mp4">
                </video> -->
                <!-- <div class="text">Demo 2: Square table assembly</div> -->
              </div>
            </div>
          
            <!-- 幻灯片 2：并排显示另外两个视频 -->
            <div class="mySlides fade">
              <div>
                <img
                    src="assets/img/what did I put in the black dustbin__distribution.gif"
                    alt="full screen gif"
                    style="width: 100%; height: 100%; object-fit: cover;"
                >
                <!-- <video autoplay muted playsinline loop preload="metadata" width="100%">
                  <source src="assets/videos/pull/grasp_bulb.mp4" type="video/mp4">
                </video> -->
                <div class="text">Demo 2: What did I put in the black dustbin?</div>
              </div>
              <div>
                <img
                    src="/Users/sunhaosen/Desktop/LV-Haystack/assets/img/what did I put in the black dustbin_.gif"
                    alt="full screen gif"
                    style="width: 100%; height: 100%; object-fit: cover;"
                >
                <!-- <video autoplay muted playsinline loop preload="metadata" width="100%">
                  <source src="assets/videos/pull/insert.mp4" type="video/mp4">
                </video> -->
                <!-- <div class="text">Demo 4: Grid manipulation (another scene)</div> -->
              </div>
            </div>
          
            <!-- “上一个 / 下一个”按钮 -->
            <a class="prev" onclick="plusSlides(-1)">&#10094;</a>
            <a class="next" onclick="plusSlides(1)">&#10095;</a>
          
          </div>
          
          <!-- 小圆点切换 -->
          <div style="text-align:center; margin-top: 10px;">
            <span class="dot" onclick="currentSlide(1)"></span>
            <span class="dot" onclick="currentSlide(2)"></span>
          </div>
          
          <script>
            let slideIndex = 1;
            showSlides(slideIndex);
          
            // 下一张/上一张
            function plusSlides(n) {
              showSlides(slideIndex += n);
            }
          
            // 跳转到第 n 张
            function currentSlide(n) {
              showSlides(slideIndex = n);
            }
          
            function showSlides(n) {
              let i;
              const slides = document.getElementsByClassName("mySlides");
              const dots = document.getElementsByClassName("dot");
          
              if (n > slides.length) { slideIndex = 1 }
              if (n < 1) { slideIndex = slides.length }
          
              // 先全部隐藏
              for (i = 0; i < slides.length; i++) {
                slides[i].style.display = "none"; 
              }
              // 移除所有指示器激活状态
              for (i = 0; i < dots.length; i++) {
                dots[i].className = dots[i].className.replace(" active", "");
              }
          
              // 显示当前的幻灯片
              slides[slideIndex - 1].style.display = "flex"; 
              // 当前指示器加上 active
              dots[slideIndex - 1].className += " active";
            }
          </script>
        
        <div id="abstract">
            <h2>🚀 Introducing <strong>T*</strong> and <strong>LV-Haystack</strong> — Our Latest Advancements in Vision-Language Models (VLMs) for Comprehensive Long Video Understanding!</h2>
            <br>
            <ul>
                <li><strong>Lightweight Plugin 🧩:</strong> T* enhances LLaVA-OV-72B from 56% to 62% and GPT-4o from 50% to 53% using just 32 frames.</li>
                <li><strong>Faster Inference ⚡:</strong> Reduced latency from 34.9 seconds to 10.4 seconds and lowered computational demand from 691 TFLOPs to 170 TFLOPs compared to the previous state-of-the-art.</li>
                <li><strong>Large-Scale Dataset 📚:</strong> LV-Haystack includes 300 hours of video content and 11,000 samples.</li>
            </ul>
            <p class="summary">→ Effective long video understanding requires fine-grained supervision. We initiate this by exploring "key frames" as the initial step 🪜.</p>
        </div>

        <div id="abstract">
            <h1>Abstract</h1>
            <p> Efficient long-form video understanding remains challenging. We introduce the “Long Video Haystack” task: finding a minimal set of relevant frames (one to five) from tens of thousands of frames for given queries. We provide LV-HAYSTACK, a new benchmark of 3,874 human-annotated instances with fine-grained metrics for keyframe search quality and efficiency. Current methods only achieve a 2.1% temporal F1 score on its LVBENCH subset, highlighting a large gap.
            <p>
                To address this, we propose <em>T*</em>, a lightweight keyframe search framework that reframes expensive temporal search as a spatial search. <em>T*</em> leverages image-based visual localization capabilities and introduces adaptive zooming-in across temporal and spatial dimensions. Under a 32-frame inference budget, <em>T*</em> boosts GPT-4o's performance from 50.5% to <strong>53.1%</strong>, and LLaVA-OneVision-OV-72B's from 55.5% to <strong>62.4%</strong> on the LongVideoBench XL subset.              
            <p>
        </div>

    </div>
</div>
<hr class="rounded">

<div></div>


<div style="text-align: center; margin-top: 20px;">
    <!-- 让图片在水平方向上居中 -->
    <img 
      src="assets/img/output.png" 
      alt="Sample Image" 
      width="600px" 
      style="display: block; margin: 0 auto;"
    >
    
    <!-- 用一个独立的块级元素来放文字说明，
         宽度和图片一样，并保证文字左对齐。-->
    <div style="
         width: 600px; 
         margin: 5px auto 0 auto; 
         text-align: left; 
         color: #555;
    ">
      <p style="font-size: 10px; text-align: justify;">
        <strong>Figure 1.</strong> Extrinsic evaluation results demonstrate how <em>T*</em> improves VLMs by selecting 8 keyframes (needle) from a large haystack, highlighting the significance of <strong>vision-centric search</strong>.
      </p>
    </div>
  </div>
  
<div id="overview">
    <h1>Method Overview</h1>
    <div style="font-family: Arial, sans-serif; text-align: justify;">
        <!-- <p style="color: #333;">What is <strong>T*</strong>?</p> -->
        <p>
            <strong>T*</strong> is an advanced temporal search framework designed to efficiently identify key frames relevant to specific queries.
            By transforming temporal search <span style="font-weight: bold;">⏱️</span> into spatial search <span style="font-weight: bold;">📍</span>, T* leverages lightweight object detectors and Visual Language Model (VLM) visual grounding techniques to streamline the process.
            T* demonstrates exceptional performance, both with and without additional training, making it a versatile and powerful tool for various applications.
        </p>
    </div>
    
    <!-- <div id="pdf-container" style="text-align: center; margin-top: 20px;">
        <iframe src="assets/pdf/frameworkv10.pdf" width="80%" height="800px"></iframe>
    </div> -->
    <div style="text-align: center; margin-top: 20px;">
        <img src="assets/img/methodv10.png" alt="Diagram illustrating the methodology of the project" width="600px" height="250px">
        <div style="text-align: left; color: #555;">
            <p style="font-size: 10px;"><strong>Figure 2.</strong> <em>T*</em> efficiently searches keyframes in long videos via adaptive temporal and spatial upsampling. First, a VLM identifies target cues from the question (Visual Grounding). Next, a Spatial Searching Model (e.g., YOLO-world) zooms in from coarse to fine frame distributions, avoiding exhaustive frame-by-frame scans. Finally, T* selects K keyframes for QA, serving as visual input to downstream VLMs.</p>
        </div>
    </div>
    
    <div style="text-align: center; margin-top: 20px;">
        <img src="assets/img/algorithm.png" alt="Sample Image" width="600px" height="200px">
    </div>
    <!-- <div class="allegrofail">
        <div class="video_container">
            <video loop autoplay muted playsinline preload="metadata">
                <source src="assets/videos/residual_policy.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>[[VIDEO_CAPTION_1]]</p>
            </div>
        </div>
    </div> -->
    <p> <em>T*</em> consistently enhances accuracy and reaches near-human oracle
        performance at 64 frames!</p>
    <div style="text-align: center; margin-top: 20px;">
        <img src="assets/pdf/Frames_vs_Acc_Line.png" alt="Sample Image" width="660px" height="420px">
        <div style="text-align: left; color: #555;">
            <p style="font-size: 10px;"><strong>Figure 3.</strong> Performance improvement with increasing search frames.</p>
        </div>
    </div>
    <h1>Experiments</h1>
    <p><strong>Evaluations on Downstream Tasks: Video QA<strong></p>
    <div style="text-align: center; margin-top: 20px;">
        <img src="assets/img/figure4.png" alt="Sample Image" width="600px" height="230px">
        <div style="text-align: justify; color: #555;">
            <p style="font-size: 10px;"><strong>Table 1.</strong>  QA accuracy (%) of <em>T*</em> used as a frame selection module for VLMs on two long-form QA benchmarks: LongVideoBench and Video-MME (without subtitles for fairness). Top-performing models (in gray) often use many more frames, making comparisons less direct. Models are ranked by their XLong video performance on LongVideoBench and total score on Video-MME, with frame counts noted. All baseline numbers come from their respective publications.</p>
        </div>
    </div>

    <h1>LV-HAYSTACK Data</h1>
    
    <div class="infographic-wrapper">
        <div class="infographic-container">
            
            <!-- 卡片1：Haystack-Ego4D -->
            <div class="info-card">
            <!-- 替换为自己的图标路径，如 icons/goggle.png -->
            <img src="assets/logos/DALL1.png" alt="Ego Icon">
            <div class="info-title">HAYSTACK-EGO4D</div>
            <div class="info-subtitle">Egocentric</div>
            <p><strong>video</strong>: 244</p>
            <p><strong>length</strong>: 101.1 h (~24.8 min/video)</p>
            <p><strong>frame</strong>: 10,910,850 (~44,717/video)</p>
            <p><strong>QA pair</strong>: 3,874 (~15.9/video)</p>
            <p><strong>keyframe</strong>: 18,370 (~4.7/question)</p>
            </div>
    
            <!-- 卡片2：Haystack-LVBench -->
            <div class="info-card">
            <!-- 替换为自己的图标路径，如 icons/allocentric.png -->
            <img src="assets/logos/DALL2.png" alt="Allocentric Icon">
            <div class="info-title">HAYSTACK-LVBENCH</div>
            <div class="info-subtitle">Allocentric</div>
            <p><strong>video</strong>: 246</p>
            <p><strong>length</strong>: 57.7 h (~14.1 min/video)</p>
            <p><strong>frame</strong>: 4,701,551 (~19,112/video)</p>
            <p><strong>QA pair</strong>: 602 (~2.4/video)</p>
            <p><strong>keyframe</strong>: 1,071 (~1.8/question)</p>
            </div>
    
        </div>
    </div>
    <h1>Evaluations on Search Utility and Efficiency</h1>
    <p>We explore disentangled evaluation of temporal search & video understanding with 6 fine-grained search metrics.
    </p>
    <div style="text-align: center; margin-top: 20px;">
        <img src="assets/img/table2.png" alt="Sample Image" width="600px" height="180px">
        <div style="text-align: justify; color: #555;">
            <p style="font-size: 10px;"><strong>Table 2.</strong> Search efficiency results on LV-HAYSTACK, covering both searching and overall video-understanding performance. Unless noted (e.g., Uniform-32), 8 frames are used for LLaVA-OneVision-72B QA. We report the model plus average call turns for grounding and matching (e.g., VideoAgent calls GPT-4 four times). <em>T*</em> achieves strong QA with minimal search cost, demonstrating efficient long-form video understanding. <sup>†</sup>FLOPs for VideoAgent exclude GPT-4 costs, due to its closed-source nature.</p>
        </div>
    </div>
    <div style="text-align: center; margin-top: 20px;">
        <img src="assets/img/table3.png" alt="Sample Image" width="600px" height="220px">
        <div style="text-align: justify; color: #555;">
            <p style="font-size: 10px;"><strong>Table 3.</strong> Searching utility on LV-HAYSTACK. The best 8-frame results are 
                <u>underlined</u>, while the best 32-frame results are <strong>bold</strong>. Increasing 
                searched frames improves recall but reduces precision (e.g., in retrieval methods). 
                Detector-based <em>T*</em> leads the 32-frame setting across multiple metrics, demonstrating 
                visual grounding plus iterative temporal searching. Although attention-based <em>T*</em> 
                performs well at 8 frames, it relies on larger foundation models, impacting efficiency.</p>
        </div>
    </div>

    <!-- <h1>[[EXPERIMENTS_TITLE]]</h1>
    <p>[[EXPERIMENTS_INTRO]]</p>

    <h1>[[RESULTS_TITLE]]</h1>
    <p>[[RESULTS_CONTENT]]</p> -->

    <!-- <h1>Failure Cases</h1>
    <p> </p>
    <div class="allegroupper">
        <video autoplay muted playsinline loop preload="metadata">
            <source src="[[FAILURE_VIDEO_1]]" type="video/mp4">
        </video>
        <video autoplay muted playsinline loop preload="metadata">
            <source src="[[FAILURE_VIDEO_2]]" type="video/mp4">
        </video>
    </div> -->
    <h1> Cases Study </h1>
    <div class="allegroupper">
        <video autoplay muted playsinline loop preload="metadata">
            <source src="assets/videos/insert_failure.mp4" type="video/mp4">
        </video>
        <video autoplay muted playsinline loop preload="metadata">
            <source src="assets/videos/bended_gripper.mp4" type="video/mp4">
        </video>
    </div>
    <div class="allegrolower">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/videos/unstable_grasp_pose.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>text</p>
            </div>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/videos/over_screw.mp4" type="video/mp4">
            </video>
        </div>
    </div>

    <h1>Conclusion</h1>
    <div style="text-align: justify;">
        <p>
            We revisit temporal search for long-form video understanding, focusing on a core challenge for state-of-the-art (SOTA) long-context vision-language models (VLMs). First, we formalize temporal search as a “Long Video Haystack” problem: finding a minimal set of relevant frames (among tens of thousands) given specific queries. To validate this, we introduce <strong>LV-HAYSTACK</strong>, featuring 3,874 human-annotated instances and fine-grained evaluation metrics for keyframe search quality and efficiency. Experimental results on <strong>LV-HAYSTACK</strong> reveal a significant gap in temporal search capabilities under SOTA keyframe selection methods. We further propose a lightweight keyframe searching framework, <em>T*</em>, reframing the costly temporal search into a spatial search task. Extensive experiments show that integrating <em>T*</em> into existing methods significantly boosts SOTA performance. We hope <strong>LV-HAYSTACK</strong> and <em>T*</em> will foster impactful algorithmic advances for efficient long-form video understanding.
        </p>
    </div>

    <h1>Acknowledgement</h1>
    <p>[[ACKNOWLEDGEMENT_CONTENT]]</p>

    <h1>BibTeX</h1>
    <p class="bibtex">[[BIBTEX_CONTENT]]</p>
    <br>
</div>
</body>
<script src="assets/js/full_screen_video.js"></script>
<script src="assets/js/carousel.js"></script>
</html>
