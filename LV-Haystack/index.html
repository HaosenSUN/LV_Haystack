<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>[[PAGE_TITLE]]</title>
    <style>
        body {
        font-family: Arial, sans-serif;
        background: #f9f9f9;
        /* æ•´ä½“å­—ä½“ç¨å¾®ç¼©å° */
        font-size: 14px;
        }
        .infographic-container {
        display: flex;
        flex-wrap: wrap;
        gap: 16px;
        justify-content: center;
        align-items: flex-start;
        }
        .info-card {
        background: #fff;
        border-radius: 10px;
        box-shadow: 0 2px 6px rgba(0,0,0,0.1);
        width: 300px;
        padding: 20px;
        text-align: left;
        }
        .info-card img {
        display: block;       /* å˜æˆå—çº§å…ƒç´ ä»¥ä¾¿ä½¿ç”¨ margin:0 auto */
        margin: 0 auto 8px;   /* æ°´å¹³å±…ä¸­ + è®¾ç½®8pxåº•éƒ¨å¤–è¾¹è· */
        width: 40px;
        height: 40px;
        object-fit: contain;
        }
        .info-title {
        margin: 8px 0 4px;
        font-size: 16px; /* åŸå…ˆ 18pxï¼Œç¨é™ */
        font-weight: bold;
        text-align: center;
        }
        .info-subtitle {
        color: #777;
        margin-bottom: 10px;
        font-size: 14px;
        text-align: center;
        }
        .info-card p {
        line-height: 0.5;   

        /* å­—ä½“é¢œè‰²ï¼ˆç¤ºä¾‹ #777 æ¯” #000 æµ…ä¸€äº›ï¼‰*/
        color: #777;        

        margin: 4px 0;      
        }

        #abstract h1, #abstract p {
            color: #000000;
            text-align: justify;
            text-justify: inter-word; 
        }
        h1 {
            color: #4E2A84;
        }
        h2 {
            color: #2c3e50;
        }
        ul {
            list-style-type: disc;
            margin-left: 20px;
        }
        li {
            margin-bottom: 10px;
        }
        .summary {
            margin-top: 20px;
            font-weight: bold;
        }
        .author-container {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 10px;
            justify-content: center;
            align-items: center;
            margin-top: 10px;
        }

        .author-name a {
            text-decoration: none;
            color: #000000;
            font-size: 14px;
        }

        .author-name a:hover {
            text-decoration: underline;
        }
        .slideshow-container {
        position: relative;
        max-width: 100%;
        margin: auto;
        overflow: hidden; /* è‹¥æœ‰è§†é¢‘æº¢å‡ºï¼Œéšè— */
        }

        /* æ¯ä¸ªå¹»ç¯ç‰‡ */
        .mySlides {
        display: flex;
        gap: 20px;
        justify-content: center;
        align-items: flex-start; /* å¦‚æœé«˜åº¦æœ‰å·®å¼‚ï¼Œå¯è®©é¡¶éƒ¨å¯¹é½ */
        }

        .mySlides > div {
        flex: 1;               /* ä¸¤åˆ—å¹³å‡åˆ† */
        max-width: 600px;      /* å¯ä»¥ç»™ä¸ªæœ€å¤§å®½åº¦é™åˆ¶ */
        box-sizing: border-box;
        }

        .mySlides video {
        width: 100%;
        height: auto;
        vertical-align: middle; /* é˜²æ­¢åº•éƒ¨å‡ºç°å‡ åƒç´ çš„ç©ºéš™ */
        }

        /* å¹»ç¯ç‰‡å†…æ–‡å­— */
        .text {
        /* å¦‚æœä¹‹å‰æœ‰ä¸‹è¿°å±æ€§ï¼Œå¯æš‚æ—¶æ³¨é‡Šæ‰æˆ–ç§»é™¤ */
        /* white-space: nowrap;
        text-overflow: ellipsis;
        overflow: hidden; */

        white-space: normal;   
        overflow: visible;  
        }

        /* ä¸Š/ä¸‹ä¸€ä¸ª æŒ‰é’® */
        .prev, .next {
        cursor: pointer;
        position: absolute;
        top: 50%;
        width: auto;
        padding: 16px;
        margin-top: -22px;
        color: white;
        font-weight: bold;
        font-size: 18px;
        border-radius: 0 3px 3px 0;
        user-select: none;
        z-index: 1;
        background-color: rgba(0, 0, 0, 0.5); /* åŠé€æ˜èƒŒæ™¯ */
        }
        .next {
        right: 0;
        border-radius: 3px 0 0 3px;
        }
        .prev:hover, .next:hover {
        background-color: rgba(0, 0, 0, 0.8);
        }

        /* å°åœ†ç‚¹æŒ‡ç¤ºå™¨æ ·å¼ */
        .dot {
        cursor: pointer;
        height: 15px; 
        width: 15px; 
        margin: 0 2px; 
        background-color: #bbb; 
        border-radius: 50%;
        display: inline-block; 
        transition: background-color 0.6s ease;
        }
        .active, .dot:hover {
        background-color: #717171;
        p {
        text-align: justify; /* ä¸¤è¾¹å¯¹é½ */
        }
        .full-page-image {
        position: relative;
        width: 100%;
        min-height: 100px; 
        /* å¦‚æœæƒ³ä¿æŒçª—å£å…¨å±é«˜åº¦ï¼Œå¯ç”¨ height: 100vh; */
        /* height: 100vh; */
        overflow: hidden;  /* é˜²æ­¢å­å…ƒç´ æº¢å‡º */
        }

        .full-page-image img {
        display: block;         /* é¿å…é»˜è®¤å†…è”å…ƒç´ ç©ºéš™ */
        width: 100%;            /* è®©å›¾ç‰‡éšçˆ¶å®¹å™¨å®½åº¦è‡ªé€‚åº” */
        height: auto;           /* ç­‰æ¯”ä¾‹ç¼©æ”¾ï¼Œä¸è‡³äºæº¢å‡º */
        object-fit: cover;      /* å¦‚æœæƒ³è®©å›¾ç‰‡å®Œå…¨è¦†ç›–å®¹å™¨ï¼Œä½†å¯èƒ½æˆªå–éƒ¨åˆ† */
        }

        .overlay {
        position: absolute; 
        top: 0; left: 0; 
        right: 0; bottom: 0;
        background: rgba(0, 0, 0, 0.3); /* åŠé€æ˜è’™å±‚ç¤ºä¾‹ */
        }

        .content {
        position: relative; /* ç¡®ä¿åœ¨ overlay ä¹‹ä¸Š */
        z-index: 1;         
        color: #fff;        /* æ–‡å­—é¢œè‰²çœ‹éœ€æ±‚ */
        /* å…¶ä»–æ’ç‰ˆã€é—´è·ç­‰ */
        }

    }
    </style>
    <link rel="stylesheet" href="assets/css/main.css">
    <link rel="apple-touch-icon" sizes="180x180" href="[[ICON_PNG]]">
    <link rel="icon" type="image/png" sizes="32x32" href="[[ICON_PNG]]">
    <link rel="icon" type="image/png" sizes="16x16" href="[[ICON_PNG]]">
    <link rel="manifest" href="/site.webmanifest">

    <meta property="og:type" content="website"/>
    <meta property="og:image" content="[[ICON_PNG]]"/>
    <meta property="og:image:type" content="image/png">
    <meta property="og:url" content="[[OG_URL]]"/>
    <meta property="og:title" content="[[OG_TITLE]]"/>
    <meta property="og:description" content="[[OG_DESCRIPTION]]"/>

    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:title" content="[[TWITTER_TITLE]]"/>
    <meta name="twitter:description" content="[[TWITTER_DESCRIPTION]]"/>
    <meta name="twitter:creator" content="[[TWITTER_CREATOR]]"/>

    <meta name="twitter:label1" content="Published at"/>
    <meta name="twitter:data1" content="[[PUBLISHED_AT]]"/>
    <meta name="twitter:label2" content="Reading time"/>
    <meta name="twitter:data2" content="[[READING_TIME]]"/>
</head>
<body>
<!-- <div class="full-page-image">
    <video id="bg-video" autoplay loop muted playsinline>
        <source src="/Users/sunhaosen/Desktop/LV-Haystack/assets/img/Demo_0108_v3.mp4" type="video/mp4">
    </video>
    <div class="overlay"></div>
    <div class="content" style="padding: 0 20px">
        <h1>Re-thinking Temporal Search for Long-Form Video Understanding</h1>
        <p>[[MAIN_SUBTEXT]]</p>
    </div>
</div> -->
<div class="full-page-image">
    <img
      src="assets/img/Demo_0108_v3.gif"
      alt="full screen gif"
      style="width: 100%; height: 100%; object-fit: cover;"
    >
    <div class="overlay"></div>
    <div class="content" style="padding: 0 20px">
        <h1>Re-thinking Temporal Search for Long-Form Video Understanding</h1>
        <p>[[MAIN_SUBTEXT]]</p>
    </div>
  </div>

<div id="title_slide">
    <div class="title_left">
        
        <h1 style="color:#4E2A84;"> Re-thinking Temporal Search for Long-Form Video Understanding </h1>
            <!-- <br> -->
        <!-- <div class="affiliation"> -->
        <!-- <p> <b> This is the template of NU-MLL-Lab paper websites. </b> </p>
        <p> Please check index.html and make sure <b>all content inside "[[]]" are replaced</b> with 
            the content of the paper or have been removed.</p>
        <p> after that, please <b>check all hyperlinks</b> by CTRL+F "src" and "href" in the html file and make sure they are correctly placed. </p>
        <p> please refer to 
            <a href="https://transic-robot.github.io/" style="color:#4E2AFF;">this website</a> 
            or 
            <a href="/index_old.html" style="color:#4E2AFF;">index_old.html</a> in this folder for the original code if you feel not sure what to fill in the pleaceholders. </p> -->
        <!-- </div class="affiliation"> -->
        <div class="author-container">
            <div class="author-name"><a href="https://jhuiye.com/" target="_blank">Jinhui Ye<sup>1 </a></div>
            <div class="author-name"><a href="https://zihanwang314.github.io/" target="_blank">Zihan Wang<sup>2 </a></div>
            <div class="author-name"><a href="https://haosensun.github.io/" target="_blank">Haosen Sun<sup>2 </a></div>
            <div class="author-name"><a href="https://keshik6.github.io/" target="_blank">Keshigeyan Chandrasegaran<sup>1 </a></div>
            <div class="author-name"><a href="https://zanedurante.github.io/" target="_blank">Zane Durante<sup>1 </a></div>
            <div class="author-name"><a href="https://ceyzaguirre4.github.io/" target="_blank">Cristobal Eyzaguirre<sup>1 </a></div>
            <div class="author-name"><a href="https://talkingtorobots.com/yonatanbisk.html" target="_blank">Yonatan Bisk<sup>3 </a></div>
            <div class="author-name"><a href="https://www.niebles.net/" target="_blank">Juan Carlos Niebles<sup>1 </a></div>
            <div class="author-name"><a href="https://profiles.stanford.edu/ehsan-adeli" target="_blank">Ehsan Adeli<sup>1 </a></div>
            <div class="author-name"><a href="https://profiles.stanford.edu/fei-fei-li/" target="_blank">Li Fei-Fei<sup>1 </a></div>
            <div class="author-name"><a href="https://jiajunwu.com/" target="_blank">Jiajun Wu<sup>1 </a></div>
            <div class="author-name"><a href="https://limanling.github.io/" target="_blank">Manling Li<sup>2 </a></div>
            
        </div>
        <br>
        <p class="affiliation">
            <!-- <sup>2</sup>The Hong Kong University of Science and Technology (Guangzhou) &nbsp;  -->
            <sup>1</sup>Stanford University &nbsp; 
            <sup>2</sup>Northwestern University &nbsp; 
            <sup>3</sup>Carnegie Mellon University &nbsp; 
            <!-- <sup>â€ </sup>Equal Contribution -->
        </p>
        <br>
        <!-- <div class="affiliation">
            <p><img src="assets/logos/Horizontal_purple_RGB.png" style="height: 20px"></p>
        </div>
        <br>
        <div class="venue">
            <p><b>Conference on Computer Vision and Pattern Recognition (CVPR) 2025</b></p>
        </div> -->
        <br>
        <div class="button-container">
            <a href="[[LINK]]" target="_blank" class="button"><i class="ai ai-arxiv"></i>&emsp14;arXiv</a>
            <a href="assets/pdf/5448_Re_thinking_Temporal_Sear.pdf" target="_blank" class="button"><i class="fa-light fa-file"></i>&emsp14;PDF</a>
            <a href="[[LINK]]" target="_blank" class="button"><i
                    class="fa-brands fa-x-twitter"></i>&emsp14;tl;dr</a>
            <a href="[[LINK]]" target="_blank" class="button"><i
                    class="fa-light fa-code"></i>&emsp14;Code</a>
            <!-- <a href="[[LINK]]" target="_blank" class="button"><i
                    class="fa-light fa-gear-code"></i>&emsp14;Sim Code</a> -->
            <a href="http://mll-4090-3.cs.northwestern.edu:8082/" target="_blank" class="button"><i
                    class="fa-light fa-robot-astromech"></i>&emsp14;Demo</a>
            <a href="https://huggingface.co/LVHaystack" target="_blank" class="button"><i
                    class="fa-light fa-face-smiling-hands"></i>&emsp14;Data</a>
        </div>
        <!-- <br>
        <div class="slideshow-container">
            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/pull/grid.mp4" type="video/mp4">
                </video>
                <div class="text">[[SLIDESHOW_TEXT_1]]
                </div>
            </div>

            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/pull/square_table.mp4" type="video/mp4">
                </video>
                <div class="text">[[SLIDESHOW_TEXT_2]]</div>
            </div>
        </div>
        
        <br> -->
        <!-- <div class="slideshow-container">
            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/pull/grid.mp4" type="video/mp4">
                </video>
                <div class="text">T<span style="font-variant-caps:all-small-caps;">RANSIC</span> for sim-to-real
                    transfer of robot manipulation policies.
                </div>
            </div>

            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/pull/square_table.mp4" type="video/mp4">
                </video>
                <div class="text">Assemble a square table.</div>
            </div>
            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/pull/square_table.mp4" type="video/mp4">
                </video>
                <div class="text">Assemble a square table.</div>
            </div>
            <div class="mySlides fade">
                <video autoplay muted playsinline loop preload="metadata" width="100%">
                    <source src="assets/videos/pull/square_table.mp4" type="video/mp4">
                </video>
                <div class="text">Assemble a square table.</div>
            </div>
            <a class="prev" onclick="plusSlides(-1)">&#10094;</a>
            <a class="next" onclick="plusSlides(1)">&#10095;</a>
        </div>
        <div style="text-align:center">
            <span class="dot" onclick="currentSlide(1)"></span>
            <span class="dot" onclick="currentSlide(2)"></span>
            <span class="dot" onclick="currentSlide(3)"></span>
            <span class="dot" onclick="currentSlide(4)"></span>
        </div> -->
        
        <div class="slideshow-container">
  
            <!-- å¹»ç¯ç‰‡ 1ï¼šå¹¶æ’æ˜¾ç¤ºä¸¤ä¸ªè§†é¢‘ -->
            <div class="mySlides fade">
              <!-- è§†é¢‘1 -->
              <div>
                <img
                    src="assets/img/distribution1.gif"
                    alt="full screen gif"
                    style="width: 100%; height: 100%; object-fit: cover;"
                >
                <!-- <video autoplay muted playsinline loop preload="metadata" width="100%">
                  <source src="assets/videos/pull/grid.mp4" type="video/mp4">
                </video> -->
                <div class="text">Demo 1: What color was the hanged jacket?</div>
              </div>
              <!-- è§†é¢‘2 -->
              <div>
                <img
                    src="/Users/sunhaosen/Desktop/LV-Haystack/assets/img/what color was the hanged jacket_.gif"
                    alt="full screen gif"
                    style="width: 100%; height: 100%; object-fit: cover;"
                >
                <!-- <video autoplay muted playsinline loop preload="metadata" width="100%">
                  <source src="assets/videos/pull/square_table.mp4" type="video/mp4">
                </video> -->
                <!-- <div class="text">Demo 2: Square table assembly</div> -->
              </div>
            </div>
          
            <!-- å¹»ç¯ç‰‡ 2ï¼šå¹¶æ’æ˜¾ç¤ºå¦å¤–ä¸¤ä¸ªè§†é¢‘ -->
            <div class="mySlides fade">
              <div>
                <img
                    src="assets/img/what did I put in the black dustbin__distribution.gif"
                    alt="full screen gif"
                    style="width: 100%; height: 100%; object-fit: cover;"
                >
                <!-- <video autoplay muted playsinline loop preload="metadata" width="100%">
                  <source src="assets/videos/pull/grasp_bulb.mp4" type="video/mp4">
                </video> -->
                <div class="text">Demo 2: What did I put in the black dustbin?</div>
              </div>
              <div>
                <img
                    src="/Users/sunhaosen/Desktop/LV-Haystack/assets/img/what did I put in the black dustbin_.gif"
                    alt="full screen gif"
                    style="width: 100%; height: 100%; object-fit: cover;"
                >
                <!-- <video autoplay muted playsinline loop preload="metadata" width="100%">
                  <source src="assets/videos/pull/insert.mp4" type="video/mp4">
                </video> -->
                <!-- <div class="text">Demo 4: Grid manipulation (another scene)</div> -->
              </div>
            </div>
          
            <!-- â€œä¸Šä¸€ä¸ª / ä¸‹ä¸€ä¸ªâ€æŒ‰é’® -->
            <a class="prev" onclick="plusSlides(-1)">&#10094;</a>
            <a class="next" onclick="plusSlides(1)">&#10095;</a>
          
          </div>
          
          <!-- å°åœ†ç‚¹åˆ‡æ¢ -->
          <div style="text-align:center; margin-top: 10px;">
            <span class="dot" onclick="currentSlide(1)"></span>
            <span class="dot" onclick="currentSlide(2)"></span>
          </div>
          
          <script>
            let slideIndex = 1;
            showSlides(slideIndex);
          
            // ä¸‹ä¸€å¼ /ä¸Šä¸€å¼ 
            function plusSlides(n) {
              showSlides(slideIndex += n);
            }
          
            // è·³è½¬åˆ°ç¬¬ n å¼ 
            function currentSlide(n) {
              showSlides(slideIndex = n);
            }
          
            function showSlides(n) {
              let i;
              const slides = document.getElementsByClassName("mySlides");
              const dots = document.getElementsByClassName("dot");
          
              if (n > slides.length) { slideIndex = 1 }
              if (n < 1) { slideIndex = slides.length }
          
              // å…ˆå…¨éƒ¨éšè—
              for (i = 0; i < slides.length; i++) {
                slides[i].style.display = "none"; 
              }
              // ç§»é™¤æ‰€æœ‰æŒ‡ç¤ºå™¨æ¿€æ´»çŠ¶æ€
              for (i = 0; i < dots.length; i++) {
                dots[i].className = dots[i].className.replace(" active", "");
              }
          
              // æ˜¾ç¤ºå½“å‰çš„å¹»ç¯ç‰‡
              slides[slideIndex - 1].style.display = "flex"; 
              // å½“å‰æŒ‡ç¤ºå™¨åŠ ä¸Š active
              dots[slideIndex - 1].className += " active";
            }
          </script>
        
        <div id="abstract">
            <h2>ğŸš€ Introducing <strong>T*</strong> and <strong>LV-Haystack</strong> â€” Our Latest Advancements in Vision-Language Models (VLMs) for Comprehensive Long Video Understanding!</h2>
            <br>
            <ul>
                <li><strong>Lightweight Plugin ğŸ§©:</strong> T* enhances LLaVA-OV-72B from 56% to 62% and GPT-4o from 50% to 53% using just 32 frames.</li>
                <li><strong>Faster Inference âš¡:</strong> Reduced latency from 34.9 seconds to 10.4 seconds and lowered computational demand from 691 TFLOPs to 170 TFLOPs compared to the previous state-of-the-art.</li>
                <li><strong>Large-Scale Dataset ğŸ“š:</strong> LV-Haystack includes 300 hours of video content and 11,000 samples.</li>
            </ul>
            <p class="summary">â†’ Effective long video understanding requires fine-grained supervision. We initiate this by exploring "key frames" as the initial step ğŸªœ.</p>
        </div>

        <div id="abstract">
            <h1>Abstract</h1>
            <p> Efficient long-form video understanding remains challenging. We introduce the â€œLong Video Haystackâ€ task: finding a minimal set of relevant frames (one to five) from tens of thousands of frames for given queries. We provide LV-HAYSTACK, a new benchmark of 3,874 human-annotated instances with fine-grained metrics for keyframe search quality and efficiency. Current methods only achieve a 2.1% temporal F1 score on its LVBENCH subset, highlighting a large gap.
            <p>
                To address this, we propose <em>T*</em>, a lightweight keyframe search framework that reframes expensive temporal search as a spatial search. <em>T*</em> leverages image-based visual localization capabilities and introduces adaptive zooming-in across temporal and spatial dimensions. Under a 32-frame inference budget, <em>T*</em> boosts GPT-4o's performance from 50.5% to <strong>53.1%</strong>, and LLaVA-OneVision-OV-72B's from 55.5% to <strong>62.4%</strong> on the LongVideoBench XL subset.              
            <p>
        </div>

    </div>
</div>
<hr class="rounded">

<div></div>


<div style="text-align: center; margin-top: 20px;">
    <!-- è®©å›¾ç‰‡åœ¨æ°´å¹³æ–¹å‘ä¸Šå±…ä¸­ -->
    <img 
      src="assets/img/output.png" 
      alt="Sample Image" 
      width="600px" 
      style="display: block; margin: 0 auto;"
    >
    
    <!-- ç”¨ä¸€ä¸ªç‹¬ç«‹çš„å—çº§å…ƒç´ æ¥æ”¾æ–‡å­—è¯´æ˜ï¼Œ
         å®½åº¦å’Œå›¾ç‰‡ä¸€æ ·ï¼Œå¹¶ä¿è¯æ–‡å­—å·¦å¯¹é½ã€‚-->
    <div style="
         width: 600px; 
         margin: 5px auto 0 auto; 
         text-align: left; 
         color: #555;
    ">
      <p style="font-size: 10px; text-align: justify;">
        <strong>Figure 1.</strong> Extrinsic evaluation results demonstrate how <em>T*</em> improves VLMs by selecting 8 keyframes (needle) from a large haystack, highlighting the significance of <strong>vision-centric search</strong>.
      </p>
    </div>
  </div>
  
<div id="overview">
    <h1>Method Overview</h1>
    <div style="font-family: Arial, sans-serif; text-align: justify;">
        <!-- <p style="color: #333;">What is <strong>T*</strong>?</p> -->
        <p>
            <strong>T*</strong> is an advanced temporal search framework designed to efficiently identify key frames relevant to specific queries.
            By transforming temporal search <span style="font-weight: bold;">â±ï¸</span> into spatial search <span style="font-weight: bold;">ğŸ“</span>, T* leverages lightweight object detectors and Visual Language Model (VLM) visual grounding techniques to streamline the process.
            T* demonstrates exceptional performance, both with and without additional training, making it a versatile and powerful tool for various applications.
        </p>
    </div>
    
    <!-- <div id="pdf-container" style="text-align: center; margin-top: 20px;">
        <iframe src="assets/pdf/frameworkv10.pdf" width="80%" height="800px"></iframe>
    </div> -->
    <div style="text-align: center; margin-top: 20px;">
        <img src="assets/img/methodv10.png" alt="Diagram illustrating the methodology of the project" width="600px" height="250px">
        <div style="text-align: left; color: #555;">
            <p style="font-size: 10px;"><strong>Figure 2.</strong> <em>T*</em> efficiently searches keyframes in long videos via adaptive temporal and spatial upsampling. First, a VLM identifies target cues from the question (Visual Grounding). Next, a Spatial Searching Model (e.g., YOLO-world) zooms in from coarse to fine frame distributions, avoiding exhaustive frame-by-frame scans. Finally, T* selects K keyframes for QA, serving as visual input to downstream VLMs.</p>
        </div>
    </div>
    
    <div style="text-align: center; margin-top: 20px;">
        <img src="assets/img/algorithm.png" alt="Sample Image" width="600px" height="200px">
    </div>
    <!-- <div class="allegrofail">
        <div class="video_container">
            <video loop autoplay muted playsinline preload="metadata">
                <source src="assets/videos/residual_policy.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>[[VIDEO_CAPTION_1]]</p>
            </div>
        </div>
    </div> -->
    <p> <em>T*</em> consistently enhances accuracy and reaches near-human oracle
        performance at 64 frames!</p>
    <div style="text-align: center; margin-top: 20px;">
        <img src="assets/pdf/Frames_vs_Acc_Line.png" alt="Sample Image" width="660px" height="420px">
        <div style="text-align: left; color: #555;">
            <p style="font-size: 10px;"><strong>Figure 3.</strong> Performance improvement with increasing search frames.</p>
        </div>
    </div>
    <h1>Experiments</h1>
    <p><strong>Evaluations on Downstream Tasks: Video QA<strong></p>
    <div style="text-align: center; margin-top: 20px;">
        <img src="assets/img/figure4.png" alt="Sample Image" width="600px" height="230px">
        <div style="text-align: justify; color: #555;">
            <p style="font-size: 10px;"><strong>Table 1.</strong>  QA accuracy (%) of <em>T*</em> used as a frame selection module for VLMs on two long-form QA benchmarks: LongVideoBench and Video-MME (without subtitles for fairness). Top-performing models (in gray) often use many more frames, making comparisons less direct. Models are ranked by their XLong video performance on LongVideoBench and total score on Video-MME, with frame counts noted. All baseline numbers come from their respective publications.</p>
        </div>
    </div>

    <h1>LV-HAYSTACK Data</h1>
    
    <div class="infographic-wrapper">
        <div class="infographic-container">
            
            <!-- å¡ç‰‡1ï¼šHaystack-Ego4D -->
            <div class="info-card">
            <!-- æ›¿æ¢ä¸ºè‡ªå·±çš„å›¾æ ‡è·¯å¾„ï¼Œå¦‚ icons/goggle.png -->
            <img src="assets/logos/DALL1.png" alt="Ego Icon">
            <div class="info-title">HAYSTACK-EGO4D</div>
            <div class="info-subtitle">Egocentric</div>
            <p><strong>video</strong>: 244</p>
            <p><strong>length</strong>: 101.1 h (~24.8 min/video)</p>
            <p><strong>frame</strong>: 10,910,850 (~44,717/video)</p>
            <p><strong>QA pair</strong>: 3,874 (~15.9/video)</p>
            <p><strong>keyframe</strong>: 18,370 (~4.7/question)</p>
            </div>
    
            <!-- å¡ç‰‡2ï¼šHaystack-LVBench -->
            <div class="info-card">
            <!-- æ›¿æ¢ä¸ºè‡ªå·±çš„å›¾æ ‡è·¯å¾„ï¼Œå¦‚ icons/allocentric.png -->
            <img src="assets/logos/DALL2.png" alt="Allocentric Icon">
            <div class="info-title">HAYSTACK-LVBENCH</div>
            <div class="info-subtitle">Allocentric</div>
            <p><strong>video</strong>: 246</p>
            <p><strong>length</strong>: 57.7 h (~14.1 min/video)</p>
            <p><strong>frame</strong>: 4,701,551 (~19,112/video)</p>
            <p><strong>QA pair</strong>: 602 (~2.4/video)</p>
            <p><strong>keyframe</strong>: 1,071 (~1.8/question)</p>
            </div>
    
        </div>
    </div>
    <h1>Evaluations on Search Utility and Efficiency</h1>
    <p>We explore disentangled evaluation of temporal search & video understanding with 6 fine-grained search metrics.
    </p>
    <div style="text-align: center; margin-top: 20px;">
        <img src="assets/img/table2.png" alt="Sample Image" width="600px" height="180px">
        <div style="text-align: justify; color: #555;">
            <p style="font-size: 10px;"><strong>Table 2.</strong> Search efficiency results on LV-HAYSTACK, covering both searching and overall video-understanding performance. Unless noted (e.g., Uniform-32), 8 frames are used for LLaVA-OneVision-72B QA. We report the model plus average call turns for grounding and matching (e.g., VideoAgent calls GPT-4 four times). <em>T*</em> achieves strong QA with minimal search cost, demonstrating efficient long-form video understanding. <sup>â€ </sup>FLOPs for VideoAgent exclude GPT-4 costs, due to its closed-source nature.</p>
        </div>
    </div>
    <div style="text-align: center; margin-top: 20px;">
        <img src="assets/img/table3.png" alt="Sample Image" width="600px" height="220px">
        <div style="text-align: justify; color: #555;">
            <p style="font-size: 10px;"><strong>Table 3.</strong> Searching utility on LV-HAYSTACK. The best 8-frame results are 
                <u>underlined</u>, while the best 32-frame results are <strong>bold</strong>. Increasing 
                searched frames improves recall but reduces precision (e.g., in retrieval methods). 
                Detector-based <em>T*</em> leads the 32-frame setting across multiple metrics, demonstrating 
                visual grounding plus iterative temporal searching. Although attention-based <em>T*</em> 
                performs well at 8 frames, it relies on larger foundation models, impacting efficiency.</p>
        </div>
    </div>

    <!-- <h1>[[EXPERIMENTS_TITLE]]</h1>
    <p>[[EXPERIMENTS_INTRO]]</p>

    <h1>[[RESULTS_TITLE]]</h1>
    <p>[[RESULTS_CONTENT]]</p> -->

    <!-- <h1>Failure Cases</h1>
    <p> </p>
    <div class="allegroupper">
        <video autoplay muted playsinline loop preload="metadata">
            <source src="[[FAILURE_VIDEO_1]]" type="video/mp4">
        </video>
        <video autoplay muted playsinline loop preload="metadata">
            <source src="[[FAILURE_VIDEO_2]]" type="video/mp4">
        </video>
    </div> -->
    <h1> Cases Study </h1>
    <div class="allegroupper">
        <video autoplay muted playsinline loop preload="metadata">
            <source src="assets/videos/insert_failure.mp4" type="video/mp4">
        </video>
        <video autoplay muted playsinline loop preload="metadata">
            <source src="assets/videos/bended_gripper.mp4" type="video/mp4">
        </video>
    </div>
    <div class="allegrolower">
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/videos/unstable_grasp_pose.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <p>text</p>
            </div>
        </div>
        <div class="video_container">
            <video autoplay muted playsinline loop preload="metadata">
                <source src="assets/videos/over_screw.mp4" type="video/mp4">
            </video>
        </div>
    </div>

    <h1>Conclusion</h1>
    <div style="text-align: justify;">
        <p>
            We revisit temporal search for long-form video understanding, focusing on a core challenge for state-of-the-art (SOTA) long-context vision-language models (VLMs). First, we formalize temporal search as a â€œLong Video Haystackâ€ problem: finding a minimal set of relevant frames (among tens of thousands) given specific queries. To validate this, we introduce <strong>LV-HAYSTACK</strong>, featuring 3,874 human-annotated instances and fine-grained evaluation metrics for keyframe search quality and efficiency. Experimental results on <strong>LV-HAYSTACK</strong> reveal a significant gap in temporal search capabilities under SOTA keyframe selection methods. We further propose a lightweight keyframe searching framework, <em>T*</em>, reframing the costly temporal search into a spatial search task. Extensive experiments show that integrating <em>T*</em> into existing methods significantly boosts SOTA performance. We hope <strong>LV-HAYSTACK</strong> and <em>T*</em> will foster impactful algorithmic advances for efficient long-form video understanding.
        </p>
    </div>

    <h1>Acknowledgement</h1>
    <p>[[ACKNOWLEDGEMENT_CONTENT]]</p>

    <h1>BibTeX</h1>
    <p class="bibtex">[[BIBTEX_CONTENT]]</p>
    <br>
</div>
</body>
<script src="assets/js/full_screen_video.js"></script>
<script src="assets/js/carousel.js"></script>
</html>
